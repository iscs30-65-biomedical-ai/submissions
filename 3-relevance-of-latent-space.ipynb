{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf1b20ee",
   "metadata": {},
   "source": [
    "## Submission 3: Relevance of Latent Space\n",
    "Trains CNN autoencoders to denoise pneumonia images based on [Medical image denoising using convolutional denoising autoencoders](https://arxiv.org/pdf/1608.04667.pdf). Contains 3 CNN autoencoders:\n",
    "1. Autoencoder for denoising noisy images with lambda = 25\n",
    "2. Autoencoder for denoising noisy images with lambda = 50\n",
    "3. Autoencoder for denoising noisy images with lambda = 75"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e68dde9",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d00f773b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.transforms.functional import to_tensor\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from skimage.util import random_noise\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ac2613",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b9627b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_ds_path = r\"datasets/pneumonia\"\n",
    "original_ds_filenames = os.listdir(original_ds_path)\n",
    "# print(original_ds_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "25e6f85d-2e5a-43a6-99ca-c241916dbb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset of clean (no noise) pneumonia images\n",
    "class CleanDataset(Dataset):\n",
    "    def __init__(self, img_dir):\n",
    "        self.img_dir = img_dir\n",
    "        self.img_labels = pd.DataFrame([filename for filename in original_ds_filenames])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Generate image filepath\n",
    "        filename = self.img_labels.iloc[idx, 0]\n",
    "        img_path = self.img_dir + \"/\" + filename\n",
    "        \n",
    "        # Read image and label/filename\n",
    "        image = Image.open(img_path)\n",
    "        label = self.img_labels.iloc[idx]\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "56171c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset of orig pneumonia images\n",
    "class OrigDataset(Dataset):\n",
    "    def __init__(self, img_dir, filenames):\n",
    "        self.img_dir = img_dir\n",
    "        self.filenames = filenames\n",
    "        self.preprocessing = transforms.Compose([\n",
    "            transforms.Resize((64, 64)),  # Resize images to 64x64\n",
    "            transforms.Grayscale(num_output_channels=1),  # Convert images to grayscale\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Generate image filepath\n",
    "        filename = self.filenames.iloc[idx, 0]\n",
    "        img_path = self.img_dir + \"/\" + filename\n",
    "\n",
    "        # Read image and label/filename\n",
    "        clean_img = Image.open(img_path)\n",
    "\n",
    "        # Apply preprocessing in DAE paper\n",
    "        clean_img = self.preprocessing(clean_img)\n",
    "\n",
    "        # Add noise\n",
    "        to_tensor = transforms.PILToTensor() \n",
    "        clean_img_tensor = to_tensor(clean_img).float()\n",
    "        \n",
    "        return clean_img_tensor, filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "0549e6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define transform\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((64, 64)),\n",
    "#     transforms.PILToTensor()\n",
    "# ])\n",
    "\n",
    "original_ds = CleanDataset(img_dir=original_ds_path)\n",
    "orig_ds = OrigDataset(img_dir = original_ds_path,\n",
    "                      filenames = original_ds.img_labels)\n",
    "\n",
    "# # Define dataloader\n",
    "# orig_dataloader = DataLoader(dataset=original_ds,\n",
    "#                               batch_size=64, \n",
    "#                               shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "3078df92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 1850 (79.98270644185041% of data)\n",
      "Test samples: 463 (20.01729355814959% of data)\n"
     ]
    }
   ],
   "source": [
    "# Get indices to split data into 80% training and 20% test data\n",
    "train_filenames, test_filenames = train_test_split(original_ds_filenames, train_size=0.8, random_state=0)\n",
    "\n",
    "# Generate training and test sets from indices\n",
    "train_ds = Subset(original_ds, train_filenames)\n",
    "test_ds = Subset(original_ds, test_filenames)\n",
    "\n",
    "print(f\"Training samples: {len(train_ds)} ({len(train_ds)/len(original_ds)*100}% of data)\")\n",
    "print(f\"Test samples: {len(test_ds)} ({len(test_ds)/len(original_ds)*100}% of data)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f06277",
   "metadata": {},
   "source": [
    "### Generating Noisy Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "9d86e782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset of noisy pneumonia images\n",
    "class NoisyDataset(Dataset):\n",
    "    def __init__(self, img_dir, filenames, noise_lambda):\n",
    "        self.img_dir = img_dir\n",
    "        self.filenames = filenames\n",
    "        self.noise_lambda = noise_lambda\n",
    "        self.preprocessing = transforms.Compose([\n",
    "            transforms.Resize((64, 64)),  # Resize images to 64x64\n",
    "            transforms.Grayscale(num_output_channels=1),  # Convert images to grayscale\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Generate image filepath\n",
    "        filename = self.filenames.iloc[idx, 0]\n",
    "        img_path = self.img_dir + \"/\" + filename\n",
    "\n",
    "        # Read image and label/filename\n",
    "        clean_img = Image.open(img_path)\n",
    "\n",
    "        # Apply preprocessing in DAE paper\n",
    "        clean_img = self.preprocessing(clean_img)\n",
    "\n",
    "        # Add noise\n",
    "        to_tensor = transforms.PILToTensor() \n",
    "        clean_img_tensor = to_tensor(clean_img).float()\n",
    "\n",
    "        noise = np.random.poisson(lam=self.noise_lambda, size=clean_img_tensor.shape).astype(np.float32)\n",
    "        noisy_img_tensor = clean_img_tensor + torch.from_numpy(noise)\n",
    "\n",
    "        noisy_img_tensor = torch.clamp(noisy_img_tensor, 0, 255)\n",
    "        \n",
    "        return noisy_img_tensor, clean_img_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "aa1a1312",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyDatasetRGB(Dataset):\n",
    "    def __init__(self, img_dir, filenames, noise_lambda):\n",
    "        self.img_dir = img_dir\n",
    "        self.filenames = filenames\n",
    "        self.noise_lambda = noise_lambda\n",
    "        self.preprocessing = transforms.Compose([\n",
    "            transforms.Resize((64, 64)),  # Resize images to 64x64\n",
    "            # transforms.Grayscale(num_output_channels=1),  # Convert images to grayscale\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Generate image filepath\n",
    "        filename = self.filenames.iloc[idx, 0]\n",
    "        img_path = self.img_dir + \"/\" + filename\n",
    "\n",
    "        # Read image and label/filename\n",
    "        clean_img = Image.open(img_path)\n",
    "\n",
    "        # Apply preprocessing in DAE paper\n",
    "        clean_img = self.preprocessing(clean_img)\n",
    "\n",
    "        # Add noise\n",
    "        to_tensor = transforms.PILToTensor() \n",
    "        clean_img_tensor = to_tensor(clean_img).float()\n",
    "\n",
    "        noise = np.random.poisson(lam=self.noise_lambda, size=clean_img_tensor.shape).astype(np.float32)\n",
    "        noisy_img_tensor = clean_img_tensor + torch.from_numpy(noise)\n",
    "\n",
    "        noisy_img_tensor = torch.clamp(noisy_img_tensor, 0, 255)\n",
    "        \n",
    "        return noisy_img_tensor, clean_img_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "4790fd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "notactuallynoisy_ds = NoisyDataset(img_dir = original_ds_path,\n",
    "                          filenames = original_ds.img_labels,\n",
    "                          noise_lambda = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "7203e57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(notactuallynoisy_ds, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "438767b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 64])\n",
      "With Noise (Left) vs Without Noise (Right) in noisy dataset with lambda = 75\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display images with and without noise\n",
    "noisy_imgs, clean_imgs = next(iter(loader))\n",
    "\n",
    "print(noisy_imgs[0].size())\n",
    "\n",
    "noisy_img = torch.reshape(noisy_imgs[0], (64, 64, 1))\n",
    "clean_img = torch.reshape(clean_imgs[0], (64, 64, 1))\n",
    "\n",
    "imgs = [noisy_img, clean_img]\n",
    "\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 2, 1\n",
    "for i in range(1, cols * rows + 1):\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(imgs[i-1].squeeze(), cmap=\"Greys\")\n",
    "\n",
    "print(\"With Noise (Left) vs Without Noise (Right) in noisy dataset with lambda = 75\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ddced7-820f-43ec-9cd0-709d20f9dcae",
   "metadata": {},
   "source": [
    "### Training CNN Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec731b6",
   "metadata": {},
   "source": [
    "1. Create a method called \"encode\" in your autoencoder model that accepts the same x (your image) but returns the the z latent space of the model. This is expected to run after you have trained the model for denoising."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "87f27155-2a27-4e5c-87f2-4f95e1980fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class CNNAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNAutoencoder, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, stride=2, padding=1),  # Example for grayscale images, change channels accordingly\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 7)\n",
    "        )\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, 7),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 16, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 1, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid()  # Use sigmoid for [0,1] scaled images\n",
    "        )\n",
    "        # ReLU\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    \n",
    "    def encode(self, x):\n",
    "        z = self.encoder(x)\n",
    "        z = self.relu(z)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "1aa54c56-b146-46b6-a000-2492a1339f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model Func\n",
    "def train_model(model, dataloader, criterion, optimizer, num_epochs=25):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        epoch_loss = running_loss / len(dataloader.dataset)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "e9a302bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Loss: 18391.2918\n",
      "Epoch 2/25, Loss: 18373.0440\n",
      "Epoch 3/25, Loss: 18372.0354\n",
      "Epoch 4/25, Loss: 18372.0198\n",
      "Epoch 5/25, Loss: 18372.0196\n",
      "Epoch 6/25, Loss: 18372.0195\n",
      "Epoch 7/25, Loss: 18372.0194\n",
      "Epoch 8/25, Loss: 18372.0193\n",
      "Epoch 9/25, Loss: 18372.0193\n",
      "Epoch 10/25, Loss: 18372.0193\n",
      "Epoch 11/25, Loss: 18372.0192\n",
      "Epoch 12/25, Loss: 18372.0192\n",
      "Epoch 13/25, Loss: 18372.0192\n",
      "Epoch 14/25, Loss: 18372.0192\n",
      "Epoch 15/25, Loss: 18372.0192\n",
      "Epoch 16/25, Loss: 18372.0192\n",
      "Epoch 17/25, Loss: 18372.0191\n",
      "Epoch 18/25, Loss: 18372.0190\n",
      "Epoch 19/25, Loss: 18372.0190\n",
      "Epoch 20/25, Loss: 18372.0190\n",
      "Epoch 21/25, Loss: 18372.0190\n",
      "Epoch 22/25, Loss: 18372.0190\n",
      "Epoch 23/25, Loss: 18372.0190\n",
      "Epoch 24/25, Loss: 18372.0190\n",
      "Epoch 25/25, Loss: 18372.0190\n"
     ]
    }
   ],
   "source": [
    "# Train Model\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Example for one set, repeat for each noise level dataset\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "model = CNNAutoencoder().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "dataloader = DataLoader(notactuallynoisy_ds, batch_size=64, shuffle=False)\n",
    "\n",
    "# Train each model\n",
    "train_model(model, dataloader, criterion, optimizer, num_epochs=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca117045",
   "metadata": {},
   "source": [
    "2. Using the encode method, create a dataset of z values from the original training set that you used and save it in a .csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "1f180781",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv \n",
    "\n",
    "# loop through the original dataset and encode each image then write to csv\n",
    "# https://www.freecodecamp.org/news/how-to-create-a-csv-file-in-python/\n",
    "with open('encoded_images.csv', 'w', newline='') as csv_file:\n",
    "  csvwriter = csv.writer(csv_file)\n",
    "  for noisy_img_tensor, clean_img_tensor in loader:\n",
    "    # print(type(data))\n",
    "    # print(data[0])\n",
    "    x = torch.Tensor(clean_img_tensor)\n",
    "    z = model.encode(x)\n",
    "    csvwriter.writerow(z) # take each row as a one element list to be converted to scalar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63829813",
   "metadata": {},
   "source": [
    "## Evaluating results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba93b19b",
   "metadata": {},
   "source": [
    "3. Research a method that determines how good your z features are. You may start with the correlation example: https://colab.research.google.com/drive/1NNH6QBlVZ4SzeLr-hDPYVEoMJ8BnAJYkLinks to an external site.Answer the following question: For this particular task,  how did you measure how good your configuration for latent space dimensionality is? Make sure your answer follows the following criteria:\n",
    "\n",
    "- Objectively sound (computationally proven)\n",
    "- Explainable"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
