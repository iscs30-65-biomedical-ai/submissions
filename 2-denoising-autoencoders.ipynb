{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf1b20ee",
   "metadata": {},
   "source": [
    "# Submission 2: Denoising Autoencoders\n",
    "Trains CNN autoencoders to denoise pneumonia images based on [Medical image denoising using convolutional denoising autoencoders](https://arxiv.org/pdf/1608.04667.pdf). Contains 3 CNN autoencoders:\n",
    "1. Autoencoder for denoising noisy images with lambda = 25\n",
    "2. Autoencoder for denoising noisy images with lambda = 50\n",
    "3. Autoencoder for denoising noisy images with lambda = 75"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e68dde9",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d00f773b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from skimage.util import random_noise\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ac2613",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9627b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_ds_path = r\"datasets/pneumonia\"\n",
    "original_ds_filenames = os.listdir(original_ds_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25e6f85d-2e5a-43a6-99ca-c241916dbb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset of clean (no noise) pneumonia images\n",
    "class CleanDataset(Dataset):\n",
    "    def __init__(self, img_dir):\n",
    "        self.img_dir = img_dir\n",
    "        self.img_labels = pd.DataFrame([filename for filename in original_ds_filenames])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Generate image filepath\n",
    "        filename = self.img_labels.iloc[idx, 0]\n",
    "        img_path = self.img_dir + \"/\" + filename\n",
    "        \n",
    "        # Read image and label/filename\n",
    "        image = Image.open(img_path)\n",
    "        label = self.img_labels.iloc[idx]\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ccf66d3-023a-4f59-94aa-81a86a268f8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "original_ds = CleanDataset(img_dir=original_ds_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3078df92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 1850 (79.98270644185041% of data)\n",
      "Test samples: 463 (20.01729355814959% of data)\n"
     ]
    }
   ],
   "source": [
    "# Get indices to split data into 80% training and 20% test data\n",
    "train_filenames, test_filenames = train_test_split(original_ds_filenames, train_size=0.8, random_state=0)\n",
    "\n",
    "# Generate training and test sets from indices\n",
    "train_ds = Subset(original_ds, train_filenames)\n",
    "test_ds = Subset(original_ds, test_filenames)\n",
    "\n",
    "print(f\"Training samples: {len(train_ds)} ({len(train_ds)/len(original_ds)*100}% of data)\")\n",
    "print(f\"Test samples: {len(test_ds)} ({len(test_ds)/len(original_ds)*100}% of data)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22ca378",
   "metadata": {},
   "source": [
    "### Generating Noisy Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c930fbd7-1c03-40b4-82ab-09e4f8f76a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset of noisy pneumonia images\n",
    "class NoisyDataset(Dataset):\n",
    "    def __init__(self, img_dir, filenames, noise_lambda):\n",
    "        self.img_dir = img_dir\n",
    "        self.filenames = filenames\n",
    "        self.noise_lambda = noise_lambda\n",
    "        self.preprocessing = transforms.Compose([\n",
    "            transforms.Resize((64, 64)),  # Resize images to 64x64\n",
    "            transforms.Grayscale(num_output_channels=1),  # Convert images to grayscale\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Generate image filepath\n",
    "        filename = self.filenames.iloc[idx, 0]\n",
    "        img_path = self.img_dir + \"/\" + filename\n",
    "\n",
    "        # Read image and label/filename\n",
    "        clean_img = Image.open(img_path)\n",
    "\n",
    "        # Apply preprocessing in DAE paper\n",
    "        clean_img = self.preprocessing(clean_img)\n",
    "\n",
    "        # Add noise\n",
    "        to_tensor = transforms.PILToTensor() \n",
    "        clean_img_tensor = to_tensor(clean_img).float()\n",
    "\n",
    "        noise = np.random.poisson(lam=self.noise_lambda, size=clean_img_tensor.shape).astype(np.float32)\n",
    "        noisy_img_tensor = clean_img_tensor + torch.from_numpy(noise)\n",
    "\n",
    "        noisy_img_tensor = torch.clamp(noisy_img_tensor, 0, 255)\n",
    "        \n",
    "        return noisy_img_tensor, clean_img_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43454455",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyDatasetRGB(Dataset):\n",
    "    def __init__(self, img_dir, filenames, noise_lambda):\n",
    "        self.img_dir = img_dir\n",
    "        self.filenames = filenames\n",
    "        self.noise_lambda = noise_lambda\n",
    "        self.preprocessing = transforms.Compose([\n",
    "            transforms.Resize((64, 64)),  # Resize images to 64x64\n",
    "            # transforms.Grayscale(num_output_channels=1),  # Convert images to grayscale\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Generate image filepath\n",
    "        filename = self.filenames.iloc[idx, 0]\n",
    "        img_path = self.img_dir + \"/\" + filename\n",
    "\n",
    "        # Read image and label/filename\n",
    "        clean_img = Image.open(img_path)\n",
    "\n",
    "        # Apply preprocessing in DAE paper\n",
    "        clean_img = self.preprocessing(clean_img)\n",
    "\n",
    "        # Add noise\n",
    "        to_tensor = transforms.PILToTensor() \n",
    "        clean_img_tensor = to_tensor(clean_img).float()\n",
    "\n",
    "        noise = np.random.poisson(lam=self.noise_lambda, size=clean_img_tensor.shape).astype(np.float32)\n",
    "        noisy_img_tensor = clean_img_tensor + torch.from_numpy(noise)\n",
    "\n",
    "        noisy_img_tensor = torch.clamp(noisy_img_tensor, 0, 255)\n",
    "        \n",
    "        return noisy_img_tensor, clean_img_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7bbe820-9d51-4d5b-b0b9-2d5f239de2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 3 sets of noisy images based on the original dataset\n",
    "noisy25_ds = NoisyDataset(img_dir = original_ds_path,\n",
    "                          filenames = original_ds.img_labels,\n",
    "                          noise_lambda = 25)\n",
    "\n",
    "noisy50_ds = NoisyDataset(img_dir = original_ds_path,\n",
    "                          filenames = original_ds.img_labels,\n",
    "                          noise_lambda = 50)\n",
    "\n",
    "noisy75_ds = NoisyDataset(img_dir = original_ds_path,\n",
    "                          filenames = original_ds.img_labels,\n",
    "                          noise_lambda = 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03b7515a",
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy25_dsRGB = NoisyDatasetRGB(img_dir = original_ds_path,\n",
    "                          filenames = original_ds.img_labels,\n",
    "                          noise_lambda = 25)\n",
    "\n",
    "noisy50_dsRGB = NoisyDatasetRGB(img_dir = original_ds_path,\n",
    "                          filenames = original_ds.img_labels,\n",
    "                          noise_lambda = 50)\n",
    "\n",
    "noisy75_dsRGB = NoisyDatasetRGB(img_dir = original_ds_path,\n",
    "                          filenames = original_ds.img_labels,\n",
    "                          noise_lambda = 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e252571-1b4d-4e9f-9b89-2768ea495ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "loader = DataLoader(noisy75_ds, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70741a57-e1f4-4bf6-8892-e7ef0d2f4680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 64])\n",
      "With Noise (Left) vs Without Noise (Right) in noisy dataset with lambda = 75\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display images with and without noise\n",
    "noisy_imgs, clean_imgs = next(iter(loader))\n",
    "\n",
    "print(noisy_imgs[0].size())\n",
    "\n",
    "noisy_img = torch.reshape(noisy_imgs[0], (64, 64, 1))\n",
    "clean_img = torch.reshape(clean_imgs[0], (64, 64, 1))\n",
    "\n",
    "imgs = [noisy_img, clean_img]\n",
    "\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 2, 1\n",
    "for i in range(1, cols * rows + 1):\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(imgs[i-1].squeeze(), cmap=\"Greys\")\n",
    "\n",
    "print(\"With Noise (Left) vs Without Noise (Right) in noisy dataset with lambda = 75\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ddced7-820f-43ec-9cd0-709d20f9dcae",
   "metadata": {},
   "source": [
    "### Training CNN Autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87f27155-2a27-4e5c-87f2-4f95e1980fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class CNNAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNAutoencoder, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, stride=2, padding=1),  # Example for grayscale images, change channels accordingly\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 7)\n",
    "        )\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, 7),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 16, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 1, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid()  # Use sigmoid for [0,1] scaled images\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1aa54c56-b146-46b6-a000-2492a1339f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model Func\n",
    "def train_model(model, dataloader, criterion, optimizer, num_epochs=25):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        epoch_loss = running_loss / len(dataloader.dataset)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c0511e40-4c07-4c1d-b047-3f1587b0438b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m dataloader_75 \u001b[38;5;241m=\u001b[39m DataLoader(noisy75_ds, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Train each model\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_25\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_25\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_25\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m train_model(model_50, dataloader_50, criterion, optimizer_50, num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m25\u001b[39m)\n\u001b[0;32m     23\u001b[0m train_model(model_75, dataloader_75, criterion, optimizer_75, num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m25\u001b[39m)\n",
      "Cell \u001b[1;32mIn[14], line 7\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, dataloader, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[0;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()  \u001b[38;5;66;03m# Set model to training mode\u001b[39;00m\n\u001b[0;32m      6\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[0;32m      8\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      9\u001b[0m     labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[6], line 24\u001b[0m, in \u001b[0;36mNoisyDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     21\u001b[0m clean_img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(img_path)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Apply preprocessing in DAE paper\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m clean_img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocessing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclean_img\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Add noise\u001b[39;00m\n\u001b[0;32m     27\u001b[0m to_tensor \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mPILToTensor() \n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torchvision\\transforms\\transforms.py:361\u001b[0m, in \u001b[0;36mResize.forward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m    354\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    355\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;124;03m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    359\u001b[0m \u001b[38;5;124;03m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 361\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mantialias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torchvision\\transforms\\functional.py:490\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[0;32m    488\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnti-alias option is always applied for PIL Image input. Argument antialias is ignored.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    489\u001b[0m     pil_interpolation \u001b[38;5;241m=\u001b[39m pil_modes_mapping[interpolation]\n\u001b[1;32m--> 490\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_pil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpil_interpolation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F_t\u001b[38;5;241m.\u001b[39mresize(img, size\u001b[38;5;241m=\u001b[39moutput_size, interpolation\u001b[38;5;241m=\u001b[39minterpolation\u001b[38;5;241m.\u001b[39mvalue, antialias\u001b[38;5;241m=\u001b[39mantialias)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torchvision\\transforms\\_functional_pil.py:250\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(size, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(size) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m):\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot inappropriate size arg: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 250\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\PIL\\Image.py:2046\u001b[0m, in \u001b[0;36mImage.resize\u001b[1;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[0;32m   2042\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreducing_gap must be 1.0 or greater\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2044\u001b[0m size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(size)\n\u001b[1;32m-> 2046\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2047\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m box \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2048\u001b[0m     box \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\PIL\\ImageFile.py:257\u001b[0m, in \u001b[0;36mImageFile.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[0;32m    252\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage file is truncated \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    253\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(b)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m bytes not processed)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    254\u001b[0m         )\n\u001b[0;32m    256\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[1;32m--> 257\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# # Train Model\n",
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# # Example for one set, repeat for each noise level dataset\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# criterion = nn.MSELoss()\n",
    "\n",
    "# model_25 = CNNAutoencoder().to(device)\n",
    "# optimizer_25 = optim.Adam(model_25.parameters(), lr=0.001)\n",
    "# dataloader_25 = DataLoader(noisy25_ds, batch_size=64, shuffle=True)\n",
    "\n",
    "# model_50 = CNNAutoencoder().to(device)\n",
    "# optimizer_50 = optim.Adam(model_50.parameters(), lr=0.001)\n",
    "# dataloader_50 = DataLoader(noisy50_ds, batch_size=64, shuffle=True)\n",
    "\n",
    "# model_75 = CNNAutoencoder().to(device)\n",
    "# optimizer_75 = optim.Adam(model_75.parameters(), lr=0.001)\n",
    "# dataloader_75 = DataLoader(noisy75_ds, batch_size=64, shuffle=True)\n",
    "\n",
    "# # Train each model\n",
    "# train_model(model_25, dataloader_25, criterion, optimizer_25, num_epochs=25)\n",
    "# train_model(model_50, dataloader_50, criterion, optimizer_50, num_epochs=25)\n",
    "# train_model(model_75, dataloader_75, criterion, optimizer_75, num_epochs=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63829813",
   "metadata": {},
   "source": [
    "## Evaluating results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba93b19b",
   "metadata": {},
   "source": [
    "Evaluate your results against the test images by running the error term you used in submission 1 against each model compared with the 4 other classical methods. The result should be the average error value for each configuration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a200be83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filters from submission 1\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as pyplot\n",
    "import numpy as np\n",
    "\n",
    "img = cv.imread(\"images/noisy.png\")\n",
    "# pyplot.imshow(img)\n",
    "\n",
    "# median filter\n",
    "def median(img):\n",
    "    kernel_size = 3\n",
    "    median_filter = cv.medianBlur(img, kernel_size)\n",
    "    # median_filter = cv.cvtColor(median_filter, cv.COLOR_BGR2GRAY)\n",
    "    return median_filter\n",
    "# pyplot.imshow(median_filter)\n",
    "\n",
    "# mean filter\n",
    "def mean(img):\n",
    "    kernel_size = [3, 3]\n",
    "    mean_filter = cv.blur(img, kernel_size)\n",
    "    # mean_filter = cv.cvtColor(mean_filter, cv.COLOR_BGR2GRAY)\n",
    "    return mean_filter\n",
    "# pyplot.imshow(mean_filter)\n",
    "\n",
    "# bilateral filter\n",
    "def bilat(img):\n",
    "    img_rgb = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n",
    "    size = 15\n",
    "    sigma_color = 75\n",
    "    sigma_space = 75\n",
    "    bilateral_filter = cv.bilateralFilter(img_rgb, size, sigma_color, sigma_space)\n",
    "    # bilateral_filter = cv.cvtColor(bilateral_filter, cv.COLOR_BGR2GRAY)\n",
    "    return bilateral_filter\n",
    "# pyplot.imshow(bilateral_filter)\n",
    "\n",
    "# gaussian blur filter\n",
    "def gauss(img):\n",
    "    kernel_size = [5, 5]\n",
    "    std_dev = 0 #passing 0 will make function auto compute std dev\n",
    "    gauss_blur = cv.GaussianBlur(img,kernel_size,std_dev)\n",
    "    # gauss_blur = cv.cvtColor(gauss_blur, cv.COLOR_BGR2GRAY)\n",
    "    return gauss_blur\n",
    "# pyplot.imshow(gauss_blur)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195501ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean squared error function from submission 1\n",
    "def mse(img_a, img_b):\n",
    "    h, w = img_a.shape\n",
    "    diff = cv.subtract(img_a, img_b)\n",
    "    err = np.sum(diff**2)\n",
    "    mse = err/(float(h*w))\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c62156",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.7.0) d:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.simd_helpers.hpp:92: error: (-2:Unspecified error) in function '__cdecl cv::impl::`anonymous-namespace'::CvtHelper<struct cv::impl::`anonymous namespace'::Set<3,4,-1>,struct cv::impl::A0x981fb336::Set<3,4,-1>,struct cv::impl::A0x981fb336::Set<0,2,5>,2>::CvtHelper(const class cv::_InputArray &,const class cv::_OutputArray &,int)'\n> Invalid number of channels in input image:\n>     'VScn::contains(scn)'\n> where\n>     'scn' is 64\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m img_median \u001b[38;5;241m=\u001b[39m median(image_c)\n\u001b[0;32m     13\u001b[0m img_mean \u001b[38;5;241m=\u001b[39m mean(image_c)\n\u001b[1;32m---> 14\u001b[0m img_bilat \u001b[38;5;241m=\u001b[39m \u001b[43mbilat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_c\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m img_gauss \u001b[38;5;241m=\u001b[39m gauss(image_c)\n\u001b[0;32m     16\u001b[0m median_eval \u001b[38;5;241m=\u001b[39m mse(image_c, img_median)\n",
      "Cell \u001b[1;32mIn[16], line 27\u001b[0m, in \u001b[0;36mbilat\u001b[1;34m(img)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbilat\u001b[39m(img):\n\u001b[1;32m---> 27\u001b[0m     img_rgb \u001b[38;5;241m=\u001b[39m \u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcvtColor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOLOR_BGR2RGB\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m     size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m15\u001b[39m\n\u001b[0;32m     29\u001b[0m     sigma_color \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m75\u001b[39m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.7.0) d:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.simd_helpers.hpp:92: error: (-2:Unspecified error) in function '__cdecl cv::impl::`anonymous-namespace'::CvtHelper<struct cv::impl::`anonymous namespace'::Set<3,4,-1>,struct cv::impl::A0x981fb336::Set<3,4,-1>,struct cv::impl::A0x981fb336::Set<0,2,5>,2>::CvtHelper(const class cv::_InputArray &,const class cv::_OutputArray &,int)'\n> Invalid number of channels in input image:\n>     'VScn::contains(scn)'\n> where\n>     'scn' is 64\n"
     ]
    }
   ],
   "source": [
    "loader = DataLoader(noisy25_dsRGB, batch_size=16)\n",
    "noisy_imgs, clean_imgs = next(iter(loader))\n",
    "\n",
    "noisy_imgs = noisy_imgs.numpy()\n",
    "clean_imgs = clean_imgs.numpy()\n",
    "\n",
    "median_score = 0\n",
    "mean_score = 0\n",
    "bilat_score = 0\n",
    "gauss_score = 0\n",
    "for image_n, image_c in zip(noisy_imgs, clean_imgs):\n",
    "    img_median = median(image_c)\n",
    "    img_mean = mean(image_c)\n",
    "    img_bilat = bilat(image_c)\n",
    "    img_gauss = gauss(image_c)\n",
    "    median_eval = mse(image_c, img_median)\n",
    "    mean_eval = mse(image_c, img_mean)\n",
    "    bilat_eval = mse(image_c, img_bilat)\n",
    "    gauss_eval = mse(image_c, img_gauss)\n",
    "    median_score += median_eval\n",
    "    mean_score += mean_eval\n",
    "    bilat_score += bilat_eval\n",
    "    gauss_score += gauss_eval\n",
    "median_score = median_score/len(noisy_imgs)\n",
    "mean_score = mean_score/len(noisy_imgs)\n",
    "bilat_score = bilat_score/len(noisy_imgs)\n",
    "gauss_score = gauss_score/len(noisy_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341e4495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use mse function\n",
    "\n",
    "median_eval = mse(img_clear, img_median)\n",
    "print('MEDIAN eval error: ', median_eval)\n",
    "\n",
    "mean_eval = mse(img_clear, img_mean)\n",
    "print('MEAN eval error: ', mean_eval)\n",
    "\n",
    "bilat_eval = mse(img_clear, img_bilat)\n",
    "print('BILATERAL eval error: ', bilat_eval)\n",
    "\n",
    "gauss_eval = mse(img_clear, img_gauss)\n",
    "print('GAUSSIAN eval error: ', gauss_eval)\n",
    "\n",
    "# model25\n",
    "\n",
    "\n",
    "# model50\n",
    "\n",
    "\n",
    "# model75\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ce62a7",
   "metadata": {},
   "source": [
    "**Why is poisson distribution the ideal one to use to simulate noise for medical images? Why not gaussian or something else? Answer in terms of relevance of medical applications.**\n",
    "\n",
    "The Poisson distribution stands out as the go-to model for simulating noise in medical images due to its close match to the random nature of photon detection, particularly in imaging methods like X-ray and PET scans. It excels in accurately representing situations with low photon counts, which are common in medical imaging, especially when using low radiation doses or capturing areas with minimal tissue density. Moreover, its ability to reflect changes in noise levels according to signal intensity mirrors the real-world behavior seen in medical images. The discrete nature of the Poisson distribution suits the whole number pixel values typical in medical imaging, and it maintains the clarity of edges better compared to the Gaussian distribution. Its solid theoretical grounding in photon counting processes also ensures that the simulated noise aligns well with the statistical properties of actual medical images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c28e18",
   "metadata": {},
   "source": [
    "**Which one performed the best? Why do you think this is the case?**\n",
    "\n",
    "The Bilaterial filter had the best balance between noise reduction and detial preservation. We believe its because it manages to preserve the edge, does selective smoothing, and is very intuitive. It's also very robust."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
